
---
title: "Land cover model for WMAs in Alabama"
output: html_document
date: "2025-04-03"
---

Loading necessary libraries

```{r}
require(sf)
require(tigris)
require(geodata)
library(dplyr)
library(colorspace)
library(FedData)
library(terra)
library(randomForest)
library(caret)
```


Loading study area shapefile

```{r}
# studyArea <- st_read("C:/Users/sza0209/OneDrive - Auburn University/Documents/MSc/Thesis/Study #1/Perdido/PerdidoShapefile.shp")

studyArea = st_read('/vsicurl/https://github.com/sza0209/Sinka-study-files/raw/refs/heads/main/PerdidoData.shp')
plot(studyArea)
```

Loading field data

```{r}
shp <- st_read("C:/Users/sza0209/OneDrive - Auburn University/Documents/MSc/Thesis/Study #1/Perdido/PerdidoData.shp")

plot(shp)        # To plot the spatial data
summary(shp)     # To get a summary of the data
head(shp)        # To view the first few rows of the data frame

# Checking the coordinates output
coords <- st_coordinates(shp)
print(head(coords))
print(dim(coords))

coords <- st_coordinates(shp)
coords <- coords[, 1:2]  # Selecting only the first two columns (X and Y)

# Check to ensure the matrix is correct
print(head(coords))
```

Getting Landsat data and spectral predictors

```{r}
landsat = rast("C:/Users/sza0209/OneDrive - Auburn University/Documents/MSc/Thesis/Study #1/Perdido/PerdidoLandsatImage2024.tif")

# Extracting the necessary bands
blue <- subset(landsat, 2)
green <- subset(landsat, 3)
red <- subset(landsat, 4)
nir <- subset(landsat, 5)

#Calculating spectral indices
ndvi <- (nir - red) / (nir + red)
names(ndvi) <- "NDVI"

savi <- ((nir - red) / (nir + red + 0.5)) * 1.5
names(savi) <- "SAVI"

evi <- 2.5 * ((nir - red) / (nir + 6 * red - 7.5 * blue + 1))
names(evi) <- "EVI"

gcvi <- (nir / green) - 1
names(gcvi) <- "GCVI"

bai <- 1 / ((0.1 - red)^2 + (0.06 - nir)^2)
names(bai) <- "BAI"


#Calculating textural indices using NIR band

library(glcm)
library(raster)

# Converting the SpatRaster band to RasterLayer for GLCM compatibility
nir_band_raster <- raster(landsat[[5]]) #landsat[[5]] is the NIR band

contrast_texture <- glcm(nir_band_raster, window = c(3, 3), statistics = "contrast")
homogeneity_texture <- glcm(nir_band_raster, window = c(3, 3), statistics = "homogeneity")
entropy_texture <- glcm(nir_band_raster, window = c(3, 3), statistics = "entropy")
mean_texture <- glcm(nir_band_raster, window = c(3, 3), statistics = "mean")
correlation_texture <- glcm(nir_band_raster, window = c(3, 3), statistics = "correlation")

contrast_texture <- rast(contrast_texture)  
homogeneity_texture <- rast(homogeneity_texture)
entropy_texture <- rast(entropy_texture)
mean_texture <- rast(mean_texture)
correlation_texture <- rast(correlation_texture)

texture_stack = c(contrast_texture, homogeneity_texture, entropy_texture, mean_texture, correlation_texture)

# Stacking the original bands and the indices together
LandsatPredictors <- c(blue, green, red, nir, ndvi, savi, evi, gcvi, bai, texture_stack)
names(LandsatPredictors) <- c("Blue", "Green", "Red", "NIR", "NDVI", "SAVI", "EVI", "GCVI", "BAI", "contrast", "homogeneity", "entropy", "mean", "correlation")
print(LandsatPredictors)
```


Getting Elevation Data for study area

```{r}
#states_data <- states(cb = TRUE)
alabama <- states_data %>% 
  filter(NAME == 'Alabama')

usaElev = elevation_30s(country='USA', path=tempdir())
alabama = st_transform(alabama, st_crs(usaElev))
alabamaElev = crop(usaElev, alabama)

# Reprojecting studyArea to match the CRS of usaElev
studyArea <- st_transform(studyArea, crs(usaElev))

# masking
studyAreaElev = mask(alabamaElev, studyArea)

plot(studyAreaElev)
summary(studyAreaElev)
```


Getting nlcd and tcc data for study area

```{r}
nlcd <- rast("C:/Users/sza0209/OneDrive - Auburn University/Documents/MSc/Thesis/Study #1/Perdido/PerdidoNLCD2021.tif")
plot(nlcd)

tcc <- rast("C:/Users/sza0209/OneDrive - Auburn University/Documents/MSc/Thesis/Study #1/Perdido/PerdidoTCC2021.tif")
plot(tcc)
```


Reprojecting the rasters to the same coordinate system (UTM)

```{r}
utm_crs <- "EPSG:32616"

# Reprojecting the study area to UTM
studyArea_utm <- st_transform(studyArea, crs = utm_crs)

# Check CRS of the study area after transformation
print(st_crs(studyArea_utm))

# Reprojecting the rasters to UTM
landsat_utm <- project(landsat, utm_crs)
studyAreaElev = project(studyAreaElev, utm_crs)
tcc_utm = project(tcc, utm_crs)
nlcd_utm = project(nlcd, utm_crs)
```


Clipping predictors to a common extent

```{r}
common_extent <- ext(landsat_utm)

studyAreaElev_aligned <- crop(studyAreaElev, common_extent)
studyAreaElev_aligned <- resample(studyAreaElev_aligned, landsat_utm)

nlcd_aligned = crop(nlcd_utm, common_extent)
nlcd_aligned = resample(nlcd_aligned, landsat_utm)

tcc_utm = crop(tcc_utm, common_extent)
tcc_utm <- resample(tcc_utm, landsat_utm)

ext(nlcd_aligned)
ext(landsat_utm)
ext(studyAreaElev_aligned)
ext(tcc_utm)
ext(LandsatPredictors)
```


Creating a reclassified raster for tcc and nlcd

```{r}
tcc <- classify(tcc_utm, rcl = matrix(c(0, 100, 1), ncol = 2, byrow = TRUE)) 

nlcd_utm_rounded <- round(nlcd_utm)
# Define reclassification matrix based on NLCD "Value"
rcl_matrix <- matrix(c(
  0, 11, 11,  # Open Water
  21, 21, 21, # Developed, Open Space
  22, 22, 22, # Developed, Low Intensity
  23, 23, 23, # Developed, Medium Intensity
  24, 24, 24, # Developed, High Intensity
  31, 31, 31, # Barren Land
  41, 41, 41, # Deciduous Forest
  42, 42, 42, # Evergreen Forest
  43, 43, 43, # Mixed Forest
  52, 52, 52, # Shrub/Scrub
  71, 71, 71, # Herbaceous
  81, 81, 81, # Hay/Pasture
  82, 82, 82, # Cultivated Crops
  90, 90, 90, # Woody Wetlands
  95, 95, 95  # Emergent Herbaceous Wetlands
), ncol = 3, byrow = TRUE)

# Apply reclassification
nlcd_reclassified <- terra::classify(nlcd_utm, rcl_matrix)

# Renaming the NLCD and TCC layers to a more descriptive name
names(nlcd_reclassified) <- "NLCD"
names(tcc) <- "TCC"

# Confirming the new layer names
print(names(nlcd_reclassified))
print(names(tcc_utm))       

nlcd_reclassified = resample(nlcd_reclassified, tcc)
```


Stacking the predictors

```{r}
predictors_stack <- c(LandsatPredictors, studyAreaElev_aligned, nlcd_reclassified, tcc) 

# Converting shp to SpatVector
shp_vect <- vect(shp)

# checking CRS of both the shapefile (SpatVector) and the raster stack
print(crs(shp_vect))
print(crs(predictors_stack))

# Transform the CRS of the shapefile to match the raster stack
shp_transformed <- project(shp_vect, crs(predictors_stack))
```


Model buildling

```{r}
predictorStack = raster::stack(predictors_stack)
print(predictorStack)  
roiTraining = shp_transformed

#Lets convert Class to factor(to assign levels to the response)
roiTraining$Name = as.factor(roiTraining$Name)
```


```{r}
unique(roiTraining$Name)
```


```{r}
predictorStack_terra <- terra::rast(predictorStack)
extract <- terra::extract(predictorStack_terra, roiTraining)
head(extract)
```


```{r}
# Add an ID column to roiTraining based on row numbers
roiTraining$ID <- 1:nrow(roiTraining)

# Retain only necessary columns: "ID" and "Name"
roiTraining_df <- roiTraining[, c("ID", "Name")]
```



```{r}
# Convert roiTraining to a data frame if needed
roiTraining_df <- as.data.frame(roiTraining_df)

# Merge extracted data with the ID and Name columns from roiTraining
extractMerged <- dplyr::inner_join(extract, roiTraining_df, by = "ID")

# Verify the merged data
head(extractMerged)

# Replace Inf and NaN with NA across all columns
extractMerged[] <- lapply(extractMerged, function(x) {
  x[!is.finite(x)] <- NA
  return(x)
})

extractMerged <- na.omit(extractMerged)
summary(extractMerged)
```


```{r}
# Define the response variable
respVar <- c("Name")

# Define the predictor variables based on your data
predVar <- c("Blue", "Green", "Red", "NIR", "NDVI", "SAVI", "EVI", "GCVI", "BAI", "contrast", "homogeneity", "entropy", "mean", "correlation", "USA_elv_msk", "NLCD", "TCC")

# Identify continuous and categorical variables
continuous_vars <- setdiff(predVar, "NLCD")

# Normalize only continuous variables in extractMerged
extractMerged_normalized <- extractMerged

for (var in continuous_vars) {
  min_val <- min(extractMerged[[var]], na.rm = TRUE)
  max_val <- max(extractMerged[[var]], na.rm = TRUE)
  extractMerged_normalized[[var]] <- (extractMerged[[var]] - min_val) / (max_val - min_val)
}

# Set categorical variable as factor (if not already)
extractMerged_normalized$NLCD <- as.factor(extractMerged$NLCD)

# Convert NLCD from factor to numeric, round the values, and convert back to factor if needed
extractMerged_normalized$NLCD <- as.numeric(as.character(extractMerged_normalized$NLCD))
extractMerged_normalized$NLCD <- round(extractMerged_normalized$NLCD)
extractMerged_normalized$NLCD <- as.factor(extractMerged_normalized$NLCD)

# View the unique values and their counts in the NLCD column
table(extractMerged_normalized$NLCD)

# Convert NLCD from factor to numeric, and then round
extractMerged_normalized$NLCD <- as.numeric(as.character(extractMerged_normalized$NLCD))

summary(extractMerged_normalized)
```


```{r}
set.seed(1234) 
trainIndex <- caret::createDataPartition(extractMerged_normalized$Name,list = FALSE,p=0.7)
trainData <- extractMerged_normalized[trainIndex,]  # 70% for training Data
testData <- extractMerged_normalized[-trainIndex,] # 30% for testing Data
```


```{r}
classCount <- trainData %>%
  dplyr::group_by(Name) %>% 
  count()

print(classCount)
```


```{r}
print(head(trainData))
```


```{r}
# Remove unused levels from the response variable in the training data
trainData$Name <- droplevels(trainData$Name)

# Verify that "UP" is no longer a level
levels(trainData$Name)
```


```{r}
# Initialize lists to store errors
oob_errors <- list()
test_errors <- list()

# Set values for number of trees and mtry to test
tree_values <- c(10, 50, 100, 200, 500, 1000)  # Different values for number of trees
mtry_values <- c(1, 2, 4, 6, 8, 10)  # Different values for mtry

# Remove the "ID" column from trainData and testData
trainData <- trainData[ , !names(trainData) %in% "ID"]
testData <- testData[ , !names(testData) %in% "ID"]

# Loop over each combination of trees and mtry values
for (mtry_val in mtry_values) {
  oob_error_list <- c()
  test_error_list <- c()
  
  for (tree_val in tree_values) {
    # Train Random Forest model with specific ntree and mtry values
    rf_model <- randomForest(Name ~ ., data = trainData, 
                             ntree = tree_val, mtry = mtry_val, importance = TRUE)
    
    # Get OOB error for the final model
    oob_error <- rf_model$err.rate[tree_val, "OOB"] * 100  # Convert to percentage
    oob_error_list <- c(oob_error_list, oob_error)
    
    # Calculate test error
    rf_predictions <- predict(rf_model, newdata = testData)
    test_error <- mean(rf_predictions != testData$Name) * 100  # Convert to percentage
    test_error_list <- c(test_error_list, test_error)
  }
  
  # Store errors for each mtry value
  oob_errors[[as.character(mtry_val)]] <- oob_error_list
  test_errors[[as.character(mtry_val)]] <- test_error_list
}

# Print the final model summary
print(rf_model)
```


```{r}
# Print the confusion matrix
rf_predictions <- predict(rf_model, newdata = testData)
confusionMatrix(rf_predictions, testData$Name)
```


```{r}
conf_matrix <- confusionMatrix(rf_predictions, testData$Name)

# Extract the confusion matrix as a table
conf_matrix_table <- conf_matrix$table

# Initialize vectors to store Producer's and User's Accuracies
producers_accuracy <- numeric(ncol(conf_matrix_table))
users_accuracy <- numeric(nrow(conf_matrix_table))

# Calculate Producer's and User's Accuracies for each class
for (i in 1:nrow(conf_matrix_table)) {
  # Producer's Accuracy for class i
  producers_accuracy[i] <- conf_matrix_table[i, i] / sum(conf_matrix_table[, i])
  
  # User's Accuracy for class i
  users_accuracy[i] <- conf_matrix_table[i, i] / sum(conf_matrix_table[i, ])
}

# Combine results into a data frame for easy viewing
accuracy_df <- data.frame(
  Class = colnames(conf_matrix_table),
  Producers_Accuracy = producers_accuracy,
  Users_Accuracy = users_accuracy
)

# Print the accuracy table
print(accuracy_df)
```


```{r}
var_importance <- rf_model$importance

# Print the variable importance
print(var_importance)

varImpPlot(rf_model, main = "Variable Importance Plot")
```

